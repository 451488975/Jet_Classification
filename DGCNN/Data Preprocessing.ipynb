{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Preprocessing\n",
    "## 1. Loading data\n",
    "There are several common datatypes: .h5 , .pkl , .json , .txt and compacted datatypes .z , .gz , .zip ,etc. <br>\n",
    "Generally, we tend to use .h5, because it takes up relatively small space."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0e3724a08c80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                    \u001b[1;31m# check keys in .h5 file, we need to read it by the key.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "path = \" \"\n",
    "f = h5py.File(path, 'r')\n",
    "f.keys()                    # check keys in .h5 file, we need to read it by the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data array\n",
    "darray = f['whatever_key_you_find'][()] \n",
    "\n",
    "# we would like to use pandas to manipulate the data.\n",
    "features = ['f1','f2','f3']         \n",
    "labels = ['l1','l2']\n",
    "data_feature = pd.DataFrame(darray, columns=features)\n",
    "data_label = pd.DataFrame(darray, columns=labels)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you are familiar with your data, there is a short cut.\n",
    "path = \" \"\n",
    "with h5py.File(path, 'r') as f:\n",
    "    darray = f['whatever_key_you_find'][()]\n",
    "    data = pd.DataFrame(darray, columns= columns_you_want)              "
   ]
  },
  {
   "source": [
    "In terms of DGCNN, we are gonna use 7 features and 5 labels( labels depend on what task you are doing) as our input."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| Features(7) | Labels(5) |\n",
    "|:---|:--- |\n",
    "|\"j1_etarel\" -- delta eta, |'J_t',|\n",
    "|\"j1_phirel\" -- delta phi, |'J_q'|\n",
    "|\"log(j1_pt)\" -- log pt, |'J_g'|\n",
    "|\"log(j1_e)\" -- log E, |'J_w'|\n",
    "|\"log(j1_ptrel)\" -- log(pt / ptjet), |'J_z'|\n",
    "|\"log(j1_erel)\" -- log(E / Ejet), |\n",
    "|\"j1_deltaR\" -- delta R|"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "j1_etarel: ration of the eta of each constituent to the eta of the jet<br>\n",
    "j1_phirel: ratio of the phi of each constituent to the phi of the jet<br>\n",
    "j1_pt: constituent pt (transverse momentum)<br>\n",
    "j1_e: constituent energy<br>\n",
    "j1_ptrel: ratio of the pT of each constituent to the pT of the jet<br>\n",
    "j1_erel: ration of the energy of each constituent of the energy of the jet<br>\n",
    "j1_deltaR: sqrt((Δeta)2 + (Δ phi)2 ) <br><br>\n",
    "j_g: gluon jet<br>\n",
    "j_q: quark jet <br>\n",
    "j_w: W boson jet <br>\n",
    "j_z: Z boson jet<br>\n",
    "j_t: Top jet<br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Excercise: \n",
    "Read out all the columns and try to understand what they are.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Feature construction\n",
    "We cannot get the log values directly from the original file, therefore a little feature construction is needed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature[\"log(j1_pt)\"] = np.log(data_feature['j1_pt'])\n",
    "data_feature[\"log(j1_e)\"] = np.log(data_feature[\"j1_e\"])\n",
    "data_feature[\"log(j1_ptrel)\"] = np.log(data_feature['j1_ptrel'])\n",
    "data_feature[\"log(j1_erel)\"] = np.log(data_feature['j1_erel'])\n",
    "\n",
    "data_feature.drop(['j1_pt','j1_e','j1_ptrel','j1_erel'],axis=1,inplace=True)"
   ]
  },
  {
   "source": [
    "Now let's combine the features and labels so that we can send it to the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([data_feature,data_label],axis=1)"
   ]
  },
  {
   "source": [
    "## 3. Downsizing jets\n",
    "In the data we got, the number of constituents contained in each jet is different, ranging from 20 to 200. While we need a fixed size as input in the machine learning process, that is to say, we need to manually specify the number of constituents for each jet. If we set nConstituents = 40, all Jets whose number of constituents is less than 40 will be zero-padded."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1) How do we identify jets\n",
    "In the data I have contacted, there are two forms: particle-based and jet-based. <br>\n",
    "\n",
    "For the particle-based data, there should be a feature help identify the data. For example \"j_index\", it tells you the unique index of a jet. Get it <a href=\"https://drive.google.com/file/d/1DCpxWbWtqU4sQwmGbZTg-4cdGAWonDKy/view?usp=sharing\">here</a>.<br>\n",
    "\n",
    "For the jet-based data, each row represents a jet, you can get specific number of constinuents by conditional slicing. Get it <a href=\"https://zenodo.org/record/2603256#.X62WkFqSmbh\">here</a>.<br>\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2) N-Constituents\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e1acb3d38e91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'j_index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "labels = labels+['j_index']\n",
    "data_label = pd.DataFrame(darray, columns=labels)\n",
    "data_all = pd.concat([data_feature, data_label],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def data_transform (nConstituents, data_all):\n",
    "    kColumns = data_all.columns.shape[0]\n",
    "\n",
    "    # we expect the output shape (mJets, nConstituents, kColumns)\n",
    "    jet_list = list(set(data_all['j_index']))\n",
    "    data_expected = []\n",
    "\n",
    "    for jet in tqdm(jet_list):\n",
    "        # Zero padding for insufficient jets. \n",
    "        # So we create a empty array and add signals in.\n",
    "        jet_frame = np.zeros((nConstituents, kColumns))\n",
    "        jet_temp = data_all[data_all['j_index']==jet].values\n",
    "        if (jet_temp.shape[0]<nConstituents):\n",
    "            for i, constituent in enumerate(jet_temp):\n",
    "                jet_frame[i] = constituent\n",
    "        else:\n",
    "            jet_frame += jet_temp[:nConstituents]\n",
    "        data_expected.append(jet_frame)\n",
    "\n",
    "    # \"j_index\" is useless for machine learning part. Drop it!\n",
    "    return np.array(data_expected[:,:,:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_transform(40, data_all)"
   ]
  },
  {
   "source": [
    "This is not the only solution or the fastest function to accomplish the goal. You can try to develop a better one. If you find a better method, please share to your collegues. Because we are gonna use this method for almost all the models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3) Excercise\n",
    "Try to think how you can get the same data shape with a jet-based data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4. Encoding\n",
    "Some times we need to encode some of the columns, for example, the label column come with value {0: Light Jet, 4: Charm Jet, 5: Bottom Jet}. we need to map it to a 3-columns data, each column represents a kind of jet. It is called One-Hot Encoding. <br>\n",
    "You should not run the following code because the data we are using do not require encoding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.get_dummies(df['The_Label_Column'],prefix='label')\n",
    "# You may want to rename the columns.\n",
    "label_df.rename(columns={'flavor_0':\"Light_Jet\",'flavor_4':'Charm_Jet','flavor_5':'Bottom_Jet'},inplace=True)"
   ]
  },
  {
   "source": [
    "Just in case you want to transform it back. You can multiply the weight for each column and sum it to get back."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'label_encoded' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f6de606d8474>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabel_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_encoded\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'label_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "label_prev = np.sum(label_encoded*[0,4,5],axis=1)"
   ]
  },
  {
   "source": [
    "### Excercise\n",
    "Create an 200-length array with random integer ranging \\[0,5\\], Encode it and then get it back."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5. Train Test Split\n",
    "We rely on the sklearn package to accomplish it. There is a build-in function.<br>\n",
    "Choose a random seed and use it for all your researches. Wanna know why? To keep Consistent input very time you run. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "source": [
    "### Excercise\n",
    "Apply the code above to your data. For further explanations for parameters, Google it!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}