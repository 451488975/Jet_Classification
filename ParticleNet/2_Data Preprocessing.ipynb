{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "06b4f7c561f76ad499c5882876df75324df0d287869e55b80005d36e4919f42b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Preprocessing\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Downsize Samples\n",
    "Constrained by computing power, we may need to reduce training samples to run. You can directly use Load_Downsize_SaveAsH5.py to create a sample that your computer can run.  For example,\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = ' '\n",
    "features = ['j_index','j1_phirel','j1_etarel','j1_pt','j1_e','j1_ptrel'] \n",
    "labels = ['j_g','j_q','j_w','j_z','j_t']\n",
    "# ratio=[.25,.25,.25,.25,1]\n",
    "size = 15000                       # number of each kind of jets\n",
    "seed = 42                          # random seed\n",
    "N_par = 20                         # number of constituents in a jet\n",
    "\n",
    "from Data_Preprocessing import Load_Downsize_SaveAsH5 as cvt\n",
    "cvt.LoadTransSave(filePath,features, labels,size=size,seed=seed)"
   ]
  },
  {
   "source": [
    "You can change the parameters. The ratio is used to control the task whether you want to do a Top Tagger or 5 Tagger.<br>\n",
    "\n",
    "### Exercise\n",
    "Load_Downsize_SaveAsH5.py will create a .h5 file. Read through the code, and figure out how it works, especially, you need to change the filePath before use!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Understand Data\n",
    "Different from DGCNN, the input of ParticleNet is a dictionary with three keys {\"mask\",\"features\",\"points\"}.\n",
    "- Mask:  An array with a shape of (N, 1, P) is used to indicate if a position is occupied by a real particle or by a zero-padded value.\n",
    "- Features: An array with a shape of (N, C, P) holds the features we are going to do the classification.\n",
    "- Points: An array with sa shape of (N, 2, P) represents the coordinates of the particles in the (eta, phi) space. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Prepare data "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the 4 features we are going to use on our model.\n",
    "features_list = ['j1_phirel','j1_etarel','j1_ptLog','j1_eLog']\n",
    "# Create a dict to store the (name, index) pair, so that we do not have to look up the list to find index.\n",
    "cols = dict(zip(features_list+['constituents_index','j_index'], [i for i in range(len(features_list)+2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h5_to_data(h5path):\n",
    "    Data = {'mask':[], 'points':[], 'features':[],'label':[]}\n",
    "    f = h5py.File(h5path,'r')\n",
    "    raw_data = np.array([f[col][()] for col in cols])\n",
    "    label_arr = f['label'][()]\n",
    "    raw_data = raw_data.transpose((1,0))\n",
    "    mask, features, points = np.zeros((N_par,1)), np.zeros((N_par,len(features_list))), np.zeros((N_par,2)) # prepare constituents list\n",
    "    for i in range(len(raw_data)):\n",
    "        cIndex = int(raw_data[i][cols['constituents_index']])\n",
    "        if cIndex >= N_par:                                                     # skip when excess N_par particles\n",
    "            continue\n",
    "        \n",
    "        mask[cIndex] = np.array([raw_data[i][cols['j1_ptLog']]])      \n",
    "        # mask[i] = np.array([1])                                    \n",
    "        points[cIndex] = np.array([raw_data[i][cols['j1_etarel']],raw_data[i][cols['j1_phirel']]])\n",
    "        features[cIndex] = np.array([raw_data[i][cols[feat]] for feat in features_list])\n",
    "                \n",
    "        if i < len(raw_data)-1:\n",
    "            if raw_data[i][cols['j_index']] != raw_data[i+1][cols['j_index']] : # save the jet before switch to another\n",
    "                Data['mask'].append(mask)\n",
    "                Data['points'].append(points)\n",
    "                Data['features'].append(features)\n",
    "                Data['label'].append(label_arr[i])\n",
    "                mask, features, points = np.zeros((N_par,1)), np.zeros((N_par,len(features_list))), np.zeros((N_par,2))  \n",
    "    f.close()\n",
    "    y = Data.pop('label')\n",
    "    return Data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.losses import categorical_crossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "h5Path = \"data_500jets_5labels.h5\"\n",
    "Data,y = h5_to_data(h5Path)\n"
   ]
  },
  {
   "source": [
    "## 4. Train Test Split\n",
    "As Usual, he default split ratio is Train:Validation:Test = 6:2:2, and the default random seed = 42"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(Data, y, rateval=.2, ratetest=.2, seed=42):\n",
    "    features_train, features_test, features_val={},{},{}\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    mask = Data[\"mask\"]\n",
    "    features = Data[\"features\"]\n",
    "    points = Data[\"points\"]\n",
    "    X_ind = [i for i in range(len(y))]\n",
    "    X_train, X_ind, y_train, y_ind = train_test_split(X_ind, y, test_size=rateval+ratetest, random_state=seed)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_ind, y_ind, test_size=(rateval/(rateval+ratetest)), random_state=seed)\n",
    "    \n",
    "    features_train['mask']=np.array([mask[i] for i in X_train])\n",
    "    features_train['features']=np.array([features[i] for i in X_train])\n",
    "    features_train['points']=np.array([points[i] for i in X_train])\n",
    "    \n",
    "    features_test['mask']=np.array([mask[i] for i in X_test])\n",
    "    features_test['features']=np.array([features[i] for i in X_test])\n",
    "    features_test['points']=np.array([points[i] for i in X_test])\n",
    "    \n",
    "    features_val['mask']=np.array([mask[i] for i in X_val])\n",
    "    features_val['features']=np.array([features[i] for i in X_val])\n",
    "    features_val['points']=np.array([points[i] for i in X_val])\n",
    "    \n",
    "    return features_train, features_val, features_test, np.array(y_train), np.array(y_val), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = splitData(Data,y)"
   ]
  },
  {
   "source": [
    "### Excercise\n",
    "1. Try to prepare a dataset used to do Top Tagging.\n",
    "2. You always want to maximize your sample number. Try to find a proper value for N_par without losing too much information while avoiding too much zero-padding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}